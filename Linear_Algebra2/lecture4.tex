%! TeX root = ./main.tex
\begin{proposition}
	\label{p_t decomposition}
	$T$ is diagonalizable $\iff$ $\exists f\in M_T$ s.t. $f$ is
	the product of different polynomials of degree 1.
\end{proposition}

Before we prove this proposition, let us take a look at the properties
of annihilating polynomials.

Since $F[x]$ is a PID, $M_T$ must be generated by one element,
namely $p_T$, the minmimal polynomial of $T$.
Thus we can WLOG assmue $f=p_T$ in the above proposition.

Speaking of polynomials and linear maps, one thing that pops
into our mind is the characteristic polynomial $f_T$. In
fact there is strong relations between $p_T$ and $f_T$:

\begin{theorem}[Cayley-Hamilton]
	\label{C-H}
    The characteristic polynomial of a linear operator $T$ is
	its annihilating polynomial, i.e. $f_T(T) = 0$.
\end{theorem}
This theorem is also true when $T$ is a matrix on a module.
To prove it more generally, we introduce the concept of modules.

\begin{definition}[Modules over commutative rings]
	Let $R$ be a commutative ring. A set $M$ is called a \vocab{module} over $R$
	or an \vocab{$R$-module} if:
	\begin{itemize}
		\item There is a binary operation (addition)
			$M\times M\to M: (\alpha,\beta)\mapsto \alpha+\beta$
			such that $M$ becomes a commutative group under this operation.
		\item There is an operation (scaling)
			$R\times M\to M: (r, \alpha)\mapsto r\alpha$ with
			assosiativity and distribution over addition (both left and right).
			We also require $1_R\alpha = \alpha$ for all  $\alpha\in M$.
	\end{itemize}
\end{definition}

\begin{example}
    A commutative group automatically has a structure of $\mathbb{Z}$-module.
	(view the group operation as addition in definition of modules)
\end{example}
\begin{example}
	Let $R = F[x]$,  $T$ a linear operator on  $V$.
	Define  $R\times V\to V: (f, \alpha)\mapsto f\alpha := f(T)\alpha$.
	We can check $V$ becomes a module over $R$.
\end{example}

We can also define matrices on a commutative ring $R$, with
addition and multiplication identical to the usual matrices.
So the determinant and characteristic polynomial make sense as well.

Note that each $m\times n$ matrix represents a homomorphism $R^m\to R^n$.

\begin{proof}[Proof of \autoref{C-H}]
    Take a basis $\mathcal{B}=\{\alpha_1,\dots,\alpha_n\}$ of $V$.
	Let $A = [T]_{\mathcal{B}}$. If we view $V$ as a $R$-module ($R=F[x]$),
	\[
		(\alpha_1,\dots,\alpha_n) A = (T\alpha_1,\dots, T\alpha_n)
		= (x\alpha_1,\dots,x\alpha_n) = (\alpha_1,\dots,\alpha_n)\cdot xI_n.
	\]
	This implies $(\alpha_1,\dots,\alpha_n) (xI_n - A) = (0,\dots,0)$.
	\begin{claim}
		If $f\in F[x]$ s.t. $\exists B\in R^{n \times n}$ s.t.
		$(xI_n - A)B = fI_n$, then  $f(T) = 0$.
	\end{claim}
	\begin{proof}[Proof of the claim]
		\[
			0 = (\alpha_1,\dots,\alpha_n)(xI_n-A)B =
			(\alpha_1,\dots,\alpha_n)\cdot fI_n = (f(T)\alpha_1,\dots,f(T)\alpha_n).
		\]
		Since $\alpha_1,\dots,\alpha_n$ is a basis, $f(T)$ must equal to $0$.
	\end{proof}

	Now it's sufficient to prove $f_T$ satisfies the condition in the claim.
	This follows from letting $B = A^\mathrm{adj}$, the adjoint matrix of $A$.
\end{proof}
\begin{remark}
    In fact this proof is derived from the proof of Nakayama's lemma,
	which is an important result in commutative algebra.
\end{remark}

As a corollary, $p_T\mid f_T$.
